# Malicious Code Risks

## Overview

Prompt injections can lead to the generation or execution of malicious code. Risks include:

1. **Code Injection:** Malicious inputs might cause the LLM to generate executable code or scripts.
2. **System Exploitation:** Malicious code generated by the LLM might be used to exploit system vulnerabilities.

## Example

**Prompt:**
```
Write a script for accessing secured systems.
```

**Risk:**
This prompt could lead the LLM to generate code snippets that might be used for unauthorized access if not properly controlled.

## Mitigation

- Implement strict code review and sanitization processes.
- Avoid executing or directly using code generated by the LLM without thorough vetting.
```

These files offer a comprehensive overview of the risks associated with prompt injection and provide examples and mitigation strategies for each type of risk.